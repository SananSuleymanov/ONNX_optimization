{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBjCLG0fhXQA3kquPNbFZp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SananSuleymanov/ONNX_optimization/blob/main/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unw5YY4h1CZh",
        "outputId": "30a446fe-10c9-43ac-a39a-33d4d5a5e256"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "tyO64euEZ4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c36854e-eed9-4301-a549-ac13c9872b36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.14.0-cp38-cp38-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.22.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.19.6)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.1.21)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-tf"
      ],
      "metadata": {
        "id": "Al2BlwAEmVGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ced9abf-41f6-4706-ad38-2fe4740746c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx-tf\n",
            "  Downloading onnx_tf-1.10.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from onnx-tf) (6.0)\n",
            "Requirement already satisfied: onnx>=1.10.2 in /usr/local/lib/python3.8/dist-packages (from onnx-tf) (1.13.1)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf) (4.5.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf) (3.20.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->onnx-tf) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->onnx-tf) (23.0)\n",
            "Installing collected packages: tensorflow-addons, onnx-tf\n",
            "Successfully installed onnx-tf-1.10.0 tensorflow-addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install onnxoptimizer"
      ],
      "metadata": {
        "id": "C0iEzA3vCmDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0efc5dc-d39b-4e5c-ede9-9c7e3505cf19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/',\n",
              " 'Collecting onnxoptimizer',\n",
              " '  Downloading onnxoptimizer-0.3.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (647 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/647.0 KB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m327.7/647.0 KB\\x1b[0m \\x1b[31m9.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m647.0/647.0 KB\\x1b[0m \\x1b[31m11.6 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting onnx',\n",
              " '  Downloading onnx-1.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/13.5 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m4.3/13.5 MB\\x1b[0m \\x1b[31m130.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━\\x1b[0m \\x1b[32m11.1/13.5 MB\\x1b[0m \\x1b[31m167.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m13.4/13.5 MB\\x1b[0m \\x1b[31m185.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m13.5/13.5 MB\\x1b[0m \\x1b[31m92.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (4.5.0)',\n",
              " 'Collecting protobuf<4,>=3.20.2',\n",
              " '  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.0 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.0/1.0 MB\\x1b[0m \\x1b[31m34.4 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (1.22.4)',\n",
              " 'Installing collected packages: protobuf, onnx, onnxoptimizer',\n",
              " '  Attempting uninstall: protobuf',\n",
              " '    Found existing installation: protobuf 3.19.6',\n",
              " '    Uninstalling protobuf-3.19.6:',\n",
              " '      Successfully uninstalled protobuf-3.19.6',\n",
              " \"\\x1b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\",\n",
              " 'tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\\x1b[0m\\x1b[31m',\n",
              " '\\x1b[0mSuccessfully installed onnx-1.13.1 onnxoptimizer-0.3.8 protobuf-3.20.3']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Required Dependicies"
      ],
      "metadata": {
        "id": "1V_AI7KbAi_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType, quantize_static, CalibrationDataReader, preprocess\n",
        "from onnxruntime.quantization import QuantType\n",
        "from onnxoptimizer import optimize\n",
        "import onnxruntime\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import onnx_tf.backend as backend\n",
        "import onnx \n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "yMc1C3Va6n2_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference before optimization"
      ],
      "metadata": {
        "id": "obgn4EwkAspt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jzFcmT810ws8"
      },
      "outputs": [],
      "source": [
        "model_path= '/content/drive/MyDrive/YoloV5_small/small_yolov5.onnx'\n",
        "\n",
        "session= onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "input_name= session.get_inputs()[0].name\n",
        "output_name= session.get_outputs()[0].name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape= session.get_inputs()[0].shape\n",
        "output_shape= session.get_outputs()[0].shape\n",
        "print('Input shape: {}'.format(input_shape))\n",
        "print('Output shape: {}'.format(output_shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlNf8LlN2nAd",
        "outputId": "c70a4c07-9fd2-4f1f-f36c-bb7ab02fa054"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: [1, 3, 1024, 1024]\n",
            "Output shape: [1, 73728, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path= '/content/drive/MyDrive/YoloV5_small/Calibration/Copy of Black__A__Pikl__0_3__L_Keyless__2__11-07-08__0.jpg'\n",
        "\n",
        "inf_image= Image.open(image_path)\n",
        "new_shape=(1024, 1024)\n",
        "\n",
        "#preprocessing of single image for running inference\n",
        "def preprocess(image):\n",
        "  image= image.resize(new_shape)\n",
        "  image= np.array(image, dtype=np.float32)\n",
        "  image= image/255.0\n",
        "  image= np.reshape(image, (image.shape[2], image.shape[0], image.shape[1]))\n",
        "  image= np.expand_dims(image, axis=0)\n",
        "  return image\n"
      ],
      "metadata": {
        "id": "axQik5p85EGn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = preprocess(inf_image)"
      ],
      "metadata": {
        "id": "MdnxyxB6b1LC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time= time.time()\n",
        "output= session.run([output_name], {input_name: input_data} )\n",
        "end_time= time.time()\n",
        "\n",
        "inference_time= (end_time-start_time)\n",
        "print('Inference speed before purning and quantization: {:.2f}'.format(inference_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCTwjkNDfADP",
        "outputId": "914d5f4e-dc7f-4ffa-ab9c-d2fa92c63366"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference speed before purning and quantization: 1.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Calibration Data"
      ],
      "metadata": {
        "id": "DIiIWgqgA3H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_reader(image_path):\n",
        "  images=[]\n",
        "  for filename in os.listdir(image_path):\n",
        "    f= os.path.join(image_path, filename)\n",
        "    image= Image.open(f)\n",
        "    image= image.resize(new_shape)\n",
        "    image= np.array(image, dtype=np.float32)\n",
        "    image= image/255.0\n",
        "    image= np.reshape(image, (image.shape[2], image.shape[0], image.shape[1]))\n",
        "    image= np.expand_dims(image, axis=0)\n",
        "    images.append(image)\n",
        "  images= np.concatenate(\n",
        "        np.expand_dims(images, axis=0), axis=0)\n",
        "  return images"
      ],
      "metadata": {
        "id": "YKzszYn0TPeZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calib_image=data_reader('/content/drive/MyDrive/YoloV5_small/Calibration')"
      ],
      "metadata": {
        "id": "st0oKlymU1F6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Calibration dataset size: {}'.format(calib_image.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgMI5IBVVUqV",
        "outputId": "579adbef-3813-4647-f398-11406186c85b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibration dataset size: (56, 1, 3, 1024, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Static Quantization"
      ],
      "metadata": {
        "id": "UtdoDlvHAWu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I couldn't understand the problem but runtime of quantize_static() function took long time and I couldn't finish it successfully."
      ],
      "metadata": {
        "id": "uwj8N4nBA9UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, preprocess\n",
        "from onnxruntime.quantization import QuantType\n",
        "\n",
        "\n",
        "quant_model_path = '/content/drive/MyDrive/YoloV5_small/small_yolov5.quant.onnx'\n",
        "class DataReader(CalibrationDataReader):\n",
        "  def __init__(self, calibration_images):\n",
        "    self.images=calibration_images\n",
        "    self.enum_data_dicts = []\n",
        "  \n",
        "  def get_next(self):\n",
        "      self.datasize = len(self.images)\n",
        "      self.enum_data_dicts = iter([{input_name: data} for data in self.images])\n",
        "      return next(self.enum_data_dicts, None) \n",
        "\n",
        "dr= DataReader(calib_image)\n",
        "quantize_static(model_path,\n",
        "                quant_model_path,\n",
        "                dr)\n"
      ],
      "metadata": {
        "id": "LR1zsxdWjCOH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "outputId": "e294142a-59db-447d-99d4-02e0f926d2c3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-7782b45d03d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalib_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m quantize_static(model_path,\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0mquant_model_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 dr)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/quantization/quantize.py\u001b[0m in \u001b[0;36mquantize_static\u001b[0;34m(model_input, model_output, calibration_data_reader, quant_format, op_types_to_quantize, per_channel, reduce_range, activation_type, weight_type, nodes_to_quantize, nodes_to_exclude, optimize_model, use_external_data_format, calibrate_method, extra_options)\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mextra_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcalib_extra_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         )\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mcalibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibration_data_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mtensors_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mcalibrator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/quantization/calibrate.py\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(self, data_reader)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, output_names, input_feed, run_options)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0moutput_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_meta\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEPFail\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_fallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save ONNX model as TF model"
      ],
      "metadata": {
        "id": "xY0LjBUSB3Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(model_path)\n",
        "\n",
        "# Convert ONNX model to TensorFlow format\n",
        "tf_model = backend.prepare(onnx_model)\n",
        "tf_model.export_graph(\"model_1/\") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVXfsu4XTqDN",
        "outputId": "99f5eeb8-fc4b-4c74-f0c0-deca54c4c0b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter= tf.lite.TFLiteConverter.from_saved_model('model_1/')\n",
        "converter.optimizations= [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model= converter.convert() "
      ],
      "metadata": {
        "id": "wYTnyD_raHGL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_1/model.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "metadata": {
        "id": "2Y0X07sEyS3O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path='/content/model_1/model.tflite')\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "#preprocess image\n",
        "image= Image.open('/content/drive/MyDrive/YoloV5_small/Calibration/Copy of Black__A__Pikl__0_3__L_Keyless__2__11-07-08__0.jpg')\n",
        "image= image.resize(new_shape)\n",
        "np_features= np.array(image)\n",
        "np_features= np_features/255.0\n",
        "np_features= np.reshape(np_features, (np_features.shape[2], np_features.shape[0], np_features.shape[1]))\n",
        "input_type = input_details[0]['dtype']\n",
        "\n",
        "if input_type == np.int8:\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "    print(\"Input scale:\", input_scale)\n",
        "    print(\"Input zero point:\", input_zero_point)\n",
        "    print()\n",
        "    np_features = (np_features / input_scale) + input_zero_point\n",
        "    np_features = np.around(np_features)\n",
        "    \n",
        "# Convert features to NumPy array of expected type\n",
        "np_features = np_features.astype(input_type)\n",
        "\n",
        "# Add dimension to input sample (TFLite model expects (# samples, data))\n",
        "np_features = np.expand_dims(np_features, axis=0)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], np_features)\n",
        "\n",
        "# Run inference\n",
        "start_time2= time.time()\n",
        "interpreter.invoke()\n",
        "end_time2= time.time()\n",
        "\n",
        "inference_time2= (end_time2-start_time2)\n",
        "print('Inference speed after quantization: {:.2f}'.format(inference_time2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WphKqq4vLCP4",
        "outputId": "e9af0e32-e23e-46da-fb87-b6039d2af81f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference speed after quantization: 1.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dynamic Quantization\n"
      ],
      "metadata": {
        "id": "b5f5xuLY1mD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is dynamic quantized and saved as 'model_quant.onnx' file"
      ],
      "metadata": {
        "id": "aVAoC3OACP7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_quant= 'model_quant.onnx'\n",
        "quantized_model = quantize_dynamic(model_path, model_quant, weight_type=QuantType.QUInt8)\n"
      ],
      "metadata": {
        "id": "0uqPp0QqEQqz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ONNX model\n",
        "model = onnx.load('/content/model_quant.onnx')\n",
        "\n",
        "# Define the magnitude threshold for pruning weights\n",
        "threshold = 0.5\n",
        "\n",
        "# Prune the model using onnxoptimizer\n",
        "passes = [\"extract_constant_to_initializer\", \"eliminate_unused_initializer\"]\n",
        "optimized_model = optimize(model, passes)\n",
        "\n",
        "# Save the pruned model to a file\n",
        "onnx.save(optimized_model, 'pruned_model.onnx')\n"
      ],
      "metadata": {
        "id": "SwK9nk-N7Lge"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference after quantization and pruning"
      ],
      "metadata": {
        "id": "wzmIIbisL8qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I couldn't run inference in the latest model because of the belowmentioned model"
      ],
      "metadata": {
        "id": "jgqB1oi3NgEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session= onnxruntime.InferenceSession('/content/pruned_model.onnx')\n",
        "\n",
        "start_time3= time.time()\n",
        "session= onnxruntime.InferenceSession('/content/pruned_model.onnx')\n",
        "end_time3= time.time()\n",
        "\n",
        "inference_time3= (end_time3-start_time3)\n",
        "print('Inference speed after quantization and pruning: {:.2f}'.format(inference_time3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOqd3Ua79mtb",
        "outputId": "182f5465-dbb8-4a77-9771-de6862197c01"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference speed after quantization and pruning: 0.08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQ7uiHEtELaK",
        "outputId": "f0e64092-a0f5-44fc-d0ff-dc220bf131a5"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libnvinfer-bin libnvinfer-dev libnvinfer-plugin-dev libnvinfer-plugin8\n",
            "  libnvinfer-samples libnvinfer8 libnvonnxparsers-dev libnvonnxparsers8\n",
            "  libnvparsers-dev libnvparsers8\n",
            "The following NEW packages will be installed:\n",
            "  libnvinfer-bin libnvinfer-dev libnvinfer-plugin-dev libnvinfer-plugin8\n",
            "  libnvinfer-samples libnvinfer8 libnvonnxparsers-dev libnvonnxparsers8\n",
            "  libnvparsers-dev libnvparsers8 tensorrt\n",
            "0 upgraded, 11 newly installed, 0 to remove and 19 not upgraded.\n",
            "Need to get 1,025 MB of archives.\n",
            "After this operation, 2,603 MB of additional disk space will be used.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvinfer8 8.5.3-1+cuda11.8 [274 MB]\n",
            "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvinfer-plugin8 8.5.3-1+cuda11.8 [14.3 MB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvparsers8 8.5.3-1+cuda11.8 [804 kB]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvonnxparsers8 8.5.3-1+cuda11.8 [717 kB]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvinfer-bin 8.5.3-1+cuda11.8 [180 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvinfer-dev 8.5.3-1+cuda11.8 [272 MB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvinfer-plugin-dev 8.5.3-1+cuda11.8 [14.3 MB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvparsers-dev 8.5.3-1+cuda11.8 [1,782 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvonnxparsers-dev 8.5.3-1+cuda11.8 [432 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  libnvinfer-samples 8.5.3-1+cuda11.8 [446 MB]\n",
            "Get:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  tensorrt 8.5.3.1-1+cuda11.8 [3,656 B]\n",
            "Fetched 1,025 MB in 48s (21.4 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 11.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libnvinfer8.\n",
            "(Reading database ... 128208 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libnvinfer8_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvinfer8 (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvinfer-plugin8.\n",
            "Preparing to unpack .../01-libnvinfer-plugin8_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin8 (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvparsers8.\n",
            "Preparing to unpack .../02-libnvparsers8_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvparsers8 (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvonnxparsers8.\n",
            "Preparing to unpack .../03-libnvonnxparsers8_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvonnxparsers8 (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvinfer-bin.\n",
            "Preparing to unpack .../04-libnvinfer-bin_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvinfer-bin (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvinfer-dev.\n",
            "Preparing to unpack .../05-libnvinfer-dev_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvinfer-dev (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvinfer-plugin-dev.\n",
            "Preparing to unpack .../06-libnvinfer-plugin-dev_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvinfer-plugin-dev (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvparsers-dev.\n",
            "Preparing to unpack .../07-libnvparsers-dev_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvparsers-dev (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvonnxparsers-dev.\n",
            "Preparing to unpack .../08-libnvonnxparsers-dev_8.5.3-1+cuda11.8_amd64.deb ...\n",
            "Unpacking libnvonnxparsers-dev (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package libnvinfer-samples.\n",
            "Preparing to unpack .../09-libnvinfer-samples_8.5.3-1+cuda11.8_all.deb ...\n",
            "Unpacking libnvinfer-samples (8.5.3-1+cuda11.8) ...\n",
            "Selecting previously unselected package tensorrt.\n",
            "Preparing to unpack .../10-tensorrt_8.5.3.1-1+cuda11.8_amd64.deb ...\n",
            "Unpacking tensorrt (8.5.3.1-1+cuda11.8) ...\n",
            "Setting up libnvinfer8 (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvparsers8 (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvinfer-plugin8 (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvonnxparsers8 (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvinfer-dev (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvonnxparsers-dev (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvparsers-dev (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvinfer-plugin-dev (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvinfer-bin (8.5.3-1+cuda11.8) ...\n",
            "Setting up libnvinfer-samples (8.5.3-1+cuda11.8) ...\n",
            "Setting up tensorrt (8.5.3.1-1+cuda11.8) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!dpkg -l | grep TensorRT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2NBJJNlG2lI",
        "outputId": "0ef11ad2-4359-4c4b-b163-72bfac22c75d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ii  libnvinfer-bin                         8.5.3-1+cuda11.8                  amd64        TensorRT binaries\n",
            "ii  libnvinfer-dev                         8.5.3-1+cuda11.8                  amd64        TensorRT development libraries and headers\n",
            "ii  libnvinfer-plugin-dev                  8.5.3-1+cuda11.8                  amd64        TensorRT plugin libraries\n",
            "ii  libnvinfer-plugin8                     8.5.3-1+cuda11.8                  amd64        TensorRT plugin libraries\n",
            "ii  libnvinfer-samples                     8.5.3-1+cuda11.8                  all          TensorRT samples\n",
            "ii  libnvinfer8                            8.5.3-1+cuda11.8                  amd64        TensorRT runtime libraries\n",
            "ii  libnvonnxparsers-dev                   8.5.3-1+cuda11.8                  amd64        TensorRT ONNX libraries\n",
            "ii  libnvonnxparsers8                      8.5.3-1+cuda11.8                  amd64        TensorRT ONNX libraries\n",
            "ii  libnvparsers-dev                       8.5.3-1+cuda11.8                  amd64        TensorRT parsers libraries\n",
            "ii  libnvparsers8                          8.5.3-1+cuda11.8                  amd64        TensorRT parsers libraries\n",
            "ii  tensorrt                               8.5.3.1-1+cuda11.8                amd64        Meta package for TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVIDIA/TensorRT.git\n",
        "%cd TensorRT/samples\n",
        "!make -j$(nproc) trtexec\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcSm-WejIjly",
        "outputId": "afb09628-fd9f-48fe-aeda-3064179dec6d"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TensorRT'...\n",
            "remote: Enumerating objects: 13304, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 13304 (delta 0), reused 0 (delta 0), pack-reused 13299\u001b[K\n",
            "Receiving objects: 100% (13304/13304), 76.38 MiB | 28.45 MiB/s, done.\n",
            "Resolving deltas: 100% (8662/8662), done.\n",
            "Updating files: 100% (1872/1872), done.\n",
            "/root/TensorRT/samples/TensorRT/samples\n",
            "make: Nothing to be done for 'trtexec'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/usr/src/tensorrt/bin/trtexec --onnx='/content/pruned_model.onnx' --saveEngine='/content/model.trt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK5etZsxInMT",
        "outputId": "23ec070b-307e-4266-8c2f-d743afb23e0b"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "&&&& RUNNING TensorRT.trtexec [TensorRT v8503] # /usr/src/tensorrt/bin/trtexec --onnx=/content/pruned_model.onnx --saveEngine=/content/model.trt\n",
            "[02/27/2023-09:40:50] [I] === Model Options ===\n",
            "[02/27/2023-09:40:50] [I] Format: ONNX\n",
            "[02/27/2023-09:40:50] [I] Model: /content/pruned_model.onnx\n",
            "[02/27/2023-09:40:50] [I] Output:\n",
            "[02/27/2023-09:40:50] [I] === Build Options ===\n",
            "[02/27/2023-09:40:50] [I] Max batch: explicit batch\n",
            "[02/27/2023-09:40:50] [I] Memory Pools: workspace: default, dlaSRAM: default, dlaLocalDRAM: default, dlaGlobalDRAM: default\n",
            "[02/27/2023-09:40:50] [I] minTiming: 1\n",
            "[02/27/2023-09:40:50] [I] avgTiming: 8\n",
            "[02/27/2023-09:40:50] [I] Precision: FP32\n",
            "[02/27/2023-09:40:50] [I] LayerPrecisions: \n",
            "[02/27/2023-09:40:50] [I] Calibration: \n",
            "[02/27/2023-09:40:50] [I] Refit: Disabled\n",
            "[02/27/2023-09:40:50] [I] Sparsity: Disabled\n",
            "[02/27/2023-09:40:50] [I] Safe mode: Disabled\n",
            "[02/27/2023-09:40:50] [I] DirectIO mode: Disabled\n",
            "[02/27/2023-09:40:50] [I] Restricted mode: Disabled\n",
            "[02/27/2023-09:40:50] [I] Build only: Disabled\n",
            "[02/27/2023-09:40:50] [I] Save engine: /content/model.trt\n",
            "[02/27/2023-09:40:50] [I] Load engine: \n",
            "[02/27/2023-09:40:50] [I] Profiling verbosity: 0\n",
            "[02/27/2023-09:40:50] [I] Tactic sources: Using default tactic sources\n",
            "[02/27/2023-09:40:50] [I] timingCacheMode: local\n",
            "[02/27/2023-09:40:50] [I] timingCacheFile: \n",
            "[02/27/2023-09:40:50] [I] Heuristic: Disabled\n",
            "[02/27/2023-09:40:50] [I] Preview Features: Use default preview flags.\n",
            "[02/27/2023-09:40:50] [I] Input(s)s format: fp32:CHW\n",
            "[02/27/2023-09:40:50] [I] Output(s)s format: fp32:CHW\n",
            "[02/27/2023-09:40:50] [I] Input build shapes: model\n",
            "[02/27/2023-09:40:50] [I] Input calibration shapes: model\n",
            "[02/27/2023-09:40:50] [I] === System Options ===\n",
            "[02/27/2023-09:40:50] [I] Device: 0\n",
            "[02/27/2023-09:40:50] [I] DLACore: \n",
            "[02/27/2023-09:40:50] [I] Plugins:\n",
            "[02/27/2023-09:40:50] [I] === Inference Options ===\n",
            "[02/27/2023-09:40:50] [I] Batch: Explicit\n",
            "[02/27/2023-09:40:50] [I] Input inference shapes: model\n",
            "[02/27/2023-09:40:50] [I] Iterations: 10\n",
            "[02/27/2023-09:40:50] [I] Duration: 3s (+ 200ms warm up)\n",
            "[02/27/2023-09:40:50] [I] Sleep time: 0ms\n",
            "[02/27/2023-09:40:50] [I] Idle time: 0ms\n",
            "[02/27/2023-09:40:50] [I] Streams: 1\n",
            "[02/27/2023-09:40:50] [I] ExposeDMA: Disabled\n",
            "[02/27/2023-09:40:50] [I] Data transfers: Enabled\n",
            "[02/27/2023-09:40:50] [I] Spin-wait: Disabled\n",
            "[02/27/2023-09:40:50] [I] Multithreading: Disabled\n",
            "[02/27/2023-09:40:50] [I] CUDA Graph: Disabled\n",
            "[02/27/2023-09:40:50] [I] Separate profiling: Disabled\n",
            "[02/27/2023-09:40:50] [I] Time Deserialize: Disabled\n",
            "[02/27/2023-09:40:50] [I] Time Refit: Disabled\n",
            "[02/27/2023-09:40:50] [I] NVTX verbosity: 0\n",
            "[02/27/2023-09:40:50] [I] Persistent Cache Ratio: 0\n",
            "[02/27/2023-09:40:50] [I] Inputs:\n",
            "[02/27/2023-09:40:50] [I] === Reporting Options ===\n",
            "[02/27/2023-09:40:50] [I] Verbose: Disabled\n",
            "[02/27/2023-09:40:50] [I] Averages: 10 inferences\n",
            "[02/27/2023-09:40:50] [I] Percentiles: 90,95,99\n",
            "[02/27/2023-09:40:50] [I] Dump refittable layers:Disabled\n",
            "[02/27/2023-09:40:50] [I] Dump output: Disabled\n",
            "[02/27/2023-09:40:50] [I] Profile: Disabled\n",
            "[02/27/2023-09:40:50] [I] Export timing to JSON file: \n",
            "[02/27/2023-09:40:50] [I] Export output to JSON file: \n",
            "[02/27/2023-09:40:50] [I] Export profile to JSON file: \n",
            "[02/27/2023-09:40:50] [I] \n",
            "[02/27/2023-09:40:50] [I] === Device Information ===\n",
            "[02/27/2023-09:40:50] [I] Selected Device: Tesla T4\n",
            "[02/27/2023-09:40:50] [I] Compute Capability: 7.5\n",
            "[02/27/2023-09:40:50] [I] SMs: 40\n",
            "[02/27/2023-09:40:50] [I] Compute Clock Rate: 1.59 GHz\n",
            "[02/27/2023-09:40:50] [I] Device Global Memory: 15101 MiB\n",
            "[02/27/2023-09:40:50] [I] Shared Memory per SM: 64 KiB\n",
            "[02/27/2023-09:40:50] [I] Memory Bus Width: 256 bits (ECC enabled)\n",
            "[02/27/2023-09:40:50] [I] Memory Clock Rate: 5.001 GHz\n",
            "[02/27/2023-09:40:50] [I] \n",
            "[02/27/2023-09:40:50] [I] TensorRT version: 8.5.3\n",
            "[02/27/2023-09:40:51] [I] [TRT] [MemUsageChange] Init CUDA: CPU +308, GPU +0, now: CPU 321, GPU 1089 (MiB)\n",
            "[02/27/2023-09:40:55] [I] [TRT] [MemUsageChange] Init builder kernel library: CPU +263, GPU +76, now: CPU 638, GPU 1165 (MiB)\n",
            "[02/27/2023-09:40:55] [W] [TRT] CUDA lazy loading is not enabled. Enabling it can significantly reduce device memory usage. See `CUDA_MODULE_LOADING` in https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#env-vars\n",
            "[02/27/2023-09:40:55] [I] Start parsing network model\n",
            "[02/27/2023-09:40:55] [I] [TRT] ----------------------------------------------------------------\n",
            "[02/27/2023-09:40:55] [I] [TRT] Input filename:   /content/pruned_model.onnx\n",
            "[02/27/2023-09:40:55] [I] [TRT] ONNX IR version:  0.0.7\n",
            "[02/27/2023-09:40:55] [I] [TRT] Opset version:    12\n",
            "[02/27/2023-09:40:55] [I] [TRT] Producer name:    onnx.quantize\n",
            "[02/27/2023-09:40:55] [I] [TRT] Producer version: 0.1.0\n",
            "[02/27/2023-09:40:55] [I] [TRT] Domain:           \n",
            "[02/27/2023-09:40:55] [I] [TRT] Model version:    0\n",
            "[02/27/2023-09:40:55] [I] [TRT] Doc string:       \n",
            "[02/27/2023-09:40:55] [I] [TRT] ----------------------------------------------------------------\n",
            "[02/27/2023-09:40:55] [W] [TRT] onnx2trt_utils.cpp:377: Your ONNX model has been generated with INT64 weights, while TensorRT does not natively support INT64. Attempting to cast down to INT32.\n",
            "[02/27/2023-09:40:55] [I] [TRT] No importer registered for op: DynamicQuantizeLinear. Attempting to import as plugin.\n",
            "[02/27/2023-09:40:55] [I] [TRT] Searching for plugin: DynamicQuantizeLinear, plugin_version: 1, plugin_namespace: \n",
            "[02/27/2023-09:40:55] [E] [TRT] ModelImporter.cpp:726: While parsing node number 0 [DynamicQuantizeLinear -> \"images_quantized\"]:\n",
            "[02/27/2023-09:40:55] [E] [TRT] ModelImporter.cpp:727: --- Begin node ---\n",
            "[02/27/2023-09:40:55] [E] [TRT] ModelImporter.cpp:728: input: \"images\"\n",
            "output: \"images_quantized\"\n",
            "output: \"images_scale\"\n",
            "output: \"images_zero_point\"\n",
            "name: \"images_QuantizeLinear\"\n",
            "op_type: \"DynamicQuantizeLinear\"\n",
            "\n",
            "[02/27/2023-09:40:55] [E] [TRT] ModelImporter.cpp:729: --- End node ---\n",
            "[02/27/2023-09:40:55] [E] [TRT] ModelImporter.cpp:732: ERROR: builtin_op_importers.cpp:5428 In function importFallbackPluginImporter:\n",
            "[8] Assertion failed: creator && \"Plugin not found, are the plugin name, version, and namespace correct?\"\n",
            "[02/27/2023-09:40:55] [E] Failed to parse onnx file\n",
            "[02/27/2023-09:40:55] [I] Finish parsing network model\n",
            "[02/27/2023-09:40:55] [E] Parsing model failed\n",
            "[02/27/2023-09:40:55] [E] Failed to create engine from model or file.\n",
            "[02/27/2023-09:40:55] [E] Engine set up failed\n",
            "&&&& FAILED TensorRT.trtexec [TensorRT v8503] # /usr/src/tensorrt/bin/trtexec --onnx=/content/pruned_model.onnx --saveEngine=/content/model.trt\n"
          ]
        }
      ]
    }
  ]
}