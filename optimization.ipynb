{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONHXTthUJbP7VnF57zy6/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SananSuleymanov/ONNX_optimization/blob/main/optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unw5YY4h1CZh",
        "outputId": "30a446fe-10c9-43ac-a39a-33d4d5a5e256"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "tyO64euEZ4ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c36854e-eed9-4301-a549-ac13c9872b36"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.14.0-cp38-cp38-manylinux_2_27_x86_64.whl (5.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.22.4)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (3.19.6)\n",
            "Collecting coloredlogs\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (1.7.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.8/dist-packages (from onnxruntime) (23.1.21)\n",
            "Collecting humanfriendly>=9.1\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 KB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.8/dist-packages (from sympy->onnxruntime) (1.2.1)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime-tools"
      ],
      "metadata": {
        "id": "lgvcODDc2J89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-model-optimization"
      ],
      "metadata": {
        "id": "t9UTpMlf5518"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-tf"
      ],
      "metadata": {
        "id": "Al2BlwAEmVGb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ced9abf-41f6-4706-ad38-2fe4740746c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx-tf\n",
            "  Downloading onnx_tf-1.10.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 KB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from onnx-tf) (6.0)\n",
            "Requirement already satisfied: onnx>=1.10.2 in /usr/local/lib/python3.8/dist-packages (from onnx-tf) (1.13.1)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf) (4.5.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.8/dist-packages (from onnx>=1.10.2->onnx-tf) (3.20.3)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->onnx-tf) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons->onnx-tf) (23.0)\n",
            "Installing collected packages: tensorflow-addons, onnx-tf\n",
            "Successfully installed onnx-tf-1.10.0 tensorflow-addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!!pip install onnxoptimizer"
      ],
      "metadata": {
        "id": "C0iEzA3vCmDp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0efc5dc-d39b-4e5c-ede9-9c7e3505cf19"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/',\n",
              " 'Collecting onnxoptimizer',\n",
              " '  Downloading onnxoptimizer-0.3.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (647 kB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/647.0 KB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m327.7/647.0 KB\\x1b[0m \\x1b[31m9.5 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m647.0/647.0 KB\\x1b[0m \\x1b[31m11.6 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hCollecting onnx',\n",
              " '  Downloading onnx-1.13.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/13.5 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m\\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m4.3/13.5 MB\\x1b[0m \\x1b[31m130.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[90m╺\\x1b[0m\\x1b[90m━━━━━━\\x1b[0m \\x1b[32m11.1/13.5 MB\\x1b[0m \\x1b[31m167.0 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m\\x1b[91m╸\\x1b[0m \\x1b[32m13.4/13.5 MB\\x1b[0m \\x1b[31m185.3 MB/s\\x1b[0m eta \\x1b[36m0:00:01\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m13.5/13.5 MB\\x1b[0m \\x1b[31m92.3 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (4.5.0)',\n",
              " 'Collecting protobuf<4,>=3.20.2',\n",
              " '  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)',\n",
              " '\\x1b[?25l     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m0.0/1.0 MB\\x1b[0m \\x1b[31m?\\x1b[0m eta \\x1b[36m-:--:--\\x1b[0m',\n",
              " '\\x1b[2K     \\x1b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\x1b[0m \\x1b[32m1.0/1.0 MB\\x1b[0m \\x1b[31m34.4 MB/s\\x1b[0m eta \\x1b[36m0:00:00\\x1b[0m',\n",
              " '\\x1b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.8/dist-packages (from onnx->onnxoptimizer) (1.22.4)',\n",
              " 'Installing collected packages: protobuf, onnx, onnxoptimizer',\n",
              " '  Attempting uninstall: protobuf',\n",
              " '    Found existing installation: protobuf 3.19.6',\n",
              " '    Uninstalling protobuf-3.19.6:',\n",
              " '      Successfully uninstalled protobuf-3.19.6',\n",
              " \"\\x1b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\",\n",
              " 'tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\\x1b[0m\\x1b[31m',\n",
              " '\\x1b[0mSuccessfully installed onnx-1.13.1 onnxoptimizer-0.3.8 protobuf-3.20.3']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Required Dependicies"
      ],
      "metadata": {
        "id": "1V_AI7KbAi_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.quantization import quantize_dynamic, QuantType, quantize_static, CalibrationDataReader, preprocess\n",
        "from onnxruntime.quantization import QuantType\n",
        "from onnxoptimizer import optimize\n",
        "import onnxruntime\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import onnx_tf.backend as backend\n",
        "import onnx \n",
        "import numpy as np\n",
        "import time\n",
        "import os"
      ],
      "metadata": {
        "id": "yMc1C3Va6n2_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference before optimization"
      ],
      "metadata": {
        "id": "obgn4EwkAspt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jzFcmT810ws8"
      },
      "outputs": [],
      "source": [
        "model_path= '/content/drive/MyDrive/YoloV5_small/small_yolov5.onnx'\n",
        "\n",
        "session= onnxruntime.InferenceSession(model_path)\n",
        "\n",
        "input_name= session.get_inputs()[0].name\n",
        "output_name= session.get_outputs()[0].name"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape= session.get_inputs()[0].shape\n",
        "output_shape= session.get_outputs()[0].shape\n",
        "print('Input shape: {}'.format(input_shape))\n",
        "print('Output shape: {}'.format(output_shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlNf8LlN2nAd",
        "outputId": "c70a4c07-9fd2-4f1f-f36c-bb7ab02fa054"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: [1, 3, 1024, 1024]\n",
            "Output shape: [1, 73728, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path= '/content/drive/MyDrive/YoloV5_small/Calibration/Copy of Black__A__Pikl__0_3__L_Keyless__2__11-07-08__0.jpg'\n",
        "\n",
        "inf_image= Image.open(image_path)\n",
        "new_shape=(1024, 1024)\n",
        "\n",
        "#preprocessing of single image for running inference\n",
        "def preprocess(image):\n",
        "  image= image.resize(new_shape)\n",
        "  image= np.array(image, dtype=np.float32)\n",
        "  image= image/255.0\n",
        "  image= np.reshape(image, (image.shape[2], image.shape[0], image.shape[1]))\n",
        "  image= np.expand_dims(image, axis=0)\n",
        "  return image\n"
      ],
      "metadata": {
        "id": "axQik5p85EGn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_data = preprocess(inf_image)"
      ],
      "metadata": {
        "id": "MdnxyxB6b1LC"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time= time.time()\n",
        "output= session.run([output_name], {input_name: input_data} )\n",
        "end_time= time.time()\n",
        "\n",
        "inference_time= (end_time-start_time)\n",
        "print('Inference speed before purning and quantization: {}'.format(inference_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCTwjkNDfADP",
        "outputId": "6af1a9a4-56f9-4289-9e77-986b875f12a2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference speed before purning and quantization: 1.024125337600708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocess Calibration Data"
      ],
      "metadata": {
        "id": "DIiIWgqgA3H7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_reader(image_path):\n",
        "  images=[]\n",
        "  for filename in os.listdir(image_path):\n",
        "    f= os.path.join(image_path, filename)\n",
        "    image= Image.open(f)\n",
        "    image= image.resize(new_shape)\n",
        "    image= np.array(image, dtype=np.float32)\n",
        "    image= image/255.0\n",
        "    image= np.reshape(image, (image.shape[2], image.shape[0], image.shape[1]))\n",
        "    image= np.expand_dims(image, axis=0)\n",
        "    images.append(image)\n",
        "  images= np.concatenate(\n",
        "        np.expand_dims(images, axis=0), axis=0)\n",
        "  return images"
      ],
      "metadata": {
        "id": "YKzszYn0TPeZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calib_image=data_reader('/content/drive/MyDrive/YoloV5_small/Calibration')"
      ],
      "metadata": {
        "id": "st0oKlymU1F6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Calibration dataset size: {}'.format(calib_image.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgMI5IBVVUqV",
        "outputId": "579adbef-3813-4647-f398-11406186c85b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibration dataset size: (56, 1, 3, 1024, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save ONNX model as TF model"
      ],
      "metadata": {
        "id": "xY0LjBUSB3Uz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(model_path)\n",
        "\n",
        "# Convert ONNX model to TensorFlow format\n",
        "tf_model = backend.prepare(onnx_model)\n",
        "tf_model.export_graph(\"model_1/\") \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVXfsu4XTqDN",
        "outputId": "99f5eeb8-fc4b-4c74-f0c0-deca54c4c0b6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
            "Instructions for updating:\n",
            "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter= tf.lite.TFLiteConverter.from_saved_model('model_1/')\n",
        "converter.optimizations= [tf.lite.Optimize.DEFAULT]\n",
        "tflite_quant_model= converter.convert() "
      ],
      "metadata": {
        "id": "wYTnyD_raHGL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_1/model.tflite', 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "metadata": {
        "id": "2Y0X07sEyS3O"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path='/content/model_1/model.tflite')\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "#preprocess image\n",
        "image= Image.open('/content/drive/MyDrive/YoloV5_small/Calibration/Copy of Black__A__Pikl__0_3__L_Keyless__2__11-07-08__0.jpg')\n",
        "image= image.resize(new_shape)\n",
        "np_features= np.array(image)\n",
        "np_features= np_features/255.0\n",
        "np_features= np.reshape(np_features, (np_features.shape[2], np_features.shape[0], np_features.shape[1]))\n",
        "input_type = input_details[0]['dtype']\n",
        "\n",
        "if input_type == np.int8:\n",
        "    input_scale, input_zero_point = input_details[0]['quantization']\n",
        "    print(\"Input scale:\", input_scale)\n",
        "    print(\"Input zero point:\", input_zero_point)\n",
        "    print()\n",
        "    np_features = (np_features / input_scale) + input_zero_point\n",
        "    np_features = np.around(np_features)\n",
        "    \n",
        "# Convert features to NumPy array of expected type\n",
        "np_features = np_features.astype(input_type)\n",
        "\n",
        "# Add dimension to input sample (TFLite model expects (# samples, data))\n",
        "np_features = np.expand_dims(np_features, axis=0)\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], np_features)\n",
        "\n",
        "# Run inference\n",
        "start_time2= time.time()\n",
        "interpreter.invoke()\n",
        "end_time2= time.time()\n",
        "\n",
        "inference_time2= (end_time2-start_time2)\n",
        "print('Inference speed after quantization: {}'.format(inference_time2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WphKqq4vLCP4",
        "outputId": "5c51954e-ec8d-4d16-d698-2f3c9ca58cbe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference speed after quantization: 1.6335320472717285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dynamic Quantization\n"
      ],
      "metadata": {
        "id": "b5f5xuLY1mD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model is dynamic quantized and saved as 'model_quant.onnx' file"
      ],
      "metadata": {
        "id": "aVAoC3OACP7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_quant= 'model_quant.onnx'\n",
        "quantized_model = quantize_dynamic(model_path, model_quant, weight_type=QuantType.QUInt8)\n"
      ],
      "metadata": {
        "id": "0uqPp0QqEQqz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ONNX model\n",
        "model = onnx.load('/content/model_quant.onnx')\n",
        "\n",
        "# Define the magnitude threshold for pruning weights\n",
        "threshold = 0.5\n",
        "\n",
        "# Prune the model using onnxoptimizer\n",
        "passes = [\"extract_constant_to_initializer\", \"eliminate_unused_initializer\"]\n",
        "optimized_model = optimize(model, passes)\n",
        "\n",
        "# Save the pruned model to a file\n",
        "onnx.save(optimized_model, 'pruned_model.onnx')\n"
      ],
      "metadata": {
        "id": "SwK9nk-N7Lge"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference after quantization and pruning"
      ],
      "metadata": {
        "id": "wzmIIbisL8qH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I couldn't run inference in the latest model because of the belowmentioned model"
      ],
      "metadata": {
        "id": "jgqB1oi3NgEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session= onnxruntime.InferenceSession('/content/pruned_model.onnx')\n",
        "\n",
        "start_time3= time.time()\n",
        "session= onnxruntime.InferenceSession('/content/pruned_model.onnx')\n",
        "end_time3= time.time()\n",
        "\n",
        "inference_time3= (end_time3-start_time3)\n",
        "print('Inference speed after quantization and pruning: {}'.format(inference_time3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOqd3Ua79mtb",
        "outputId": "5b83dc38-c8a0-4b00-cba4-e44ee14957d3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference speed after quantization and pruning: 0.0789027214050293\n"
          ]
        }
      ]
    }
  ]
}